# Apache Airflow Helm Chart Values
# For use with: helm install airflow apache-airflow/airflow

# Executor: KubernetesExecutor (each task runs in its own pod)
executor: "KubernetesExecutor"

# Airflow images
images:
  airflow:
    repository: apache/airflow
    tag: "2.8.1-python3.11"
    pullPolicy: IfNotPresent

# PostgreSQL for Airflow metadata database
# Use existing Postgres from Phase 1 instead of bundled one
postgresql:
  enabled: false  # Disable bundled Postgres

# External database configuration (use existing Postgres from Phase 1)
data:
  metadataConnection:
    user: postgres
    pass: postgres
    protocol: postgresql
    host: postgres.postgres.svc.cluster.local
    port: 5432
    db: airflow  # Create separate DB for Airflow metadata
    sslmode: disable

# Redis (not needed for KubernetesExecutor)
redis:
  enabled: false

# Webserver configuration
webserver:
  replicas: 1
  service:
    type: ClusterIP
    ports:
      - name: airflow-ui
        port: 8080
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  defaultUser:
    enabled: true
    username: admin
    password: admin
    email: admin@example.com
    firstName: Admin
    lastName: User
    role: Admin

# Scheduler configuration
scheduler:
  replicas: 1
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      cpu: 1000m
      memory: 1Gi
  logGroomerSidecar:
    enabled: true
    resources:
      requests:
        cpu: 50m
        memory: 128Mi

# Triggerer (for deferrable operators)
triggerer:
  enabled: true
  replicas: 1
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 500m
      memory: 512Mi

# Workers (not used with KubernetesExecutor)
workers:
  enabled: false

# Airflow configuration
config:
  core:
    default_timezone: "UTC"
    load_examples: "False"  # Don't load example DAGs
    parallelism: 32
    max_active_tasks_per_dag: 16
    max_active_runs_per_dag: 3
    dags_are_paused_at_creation: "True"
  
  webserver:
    expose_config: "True"
    dag_default_view: "graph"
    dag_orientation: "LR"
  
  logging:
    logging_level: "INFO"
    remote_logging: "False"  # Enable later with MinIO
  
  metrics:
    statsd_on: "True"
    statsd_host: "prometheus-statsd-exporter.monitoring.svc.cluster.local"
    statsd_port: 9125
    statsd_prefix: "airflow"
  
  scheduler:
    dag_dir_list_interval: 30  # Scan DAGs every 30s
    min_file_process_interval: 30

# Environment variables
env:
  - name: AIRFLOW_CONN_POSTGRES_IOT
    value: "postgresql://postgres:postgres@postgres.postgres.svc.cluster.local:5432/iot_ops"
  - name: AIRFLOW_CONN_MINIO
    value: "aws://minioadmin:minioadmin@?endpoint_url=http://minio.minio.svc.cluster.local:9000"
  - name: AIRFLOW__CORE__DAGS_FOLDER
    value: "/opt/airflow/dags"

# Extra pip packages to install
extraPipPackages:
  - "apache-airflow-providers-postgres==5.10.0"
  - "apache-airflow-providers-amazon==8.16.0"
  - "duckdb==0.9.2"
  - "pyarrow==14.0.1"
  - "pandas==2.1.4"
  - "sqlalchemy==2.0.25"
  - "boto3==1.34.34"
  - "psycopg2-binary==2.9.9"

# DAGs configuration
dags:
  persistence:
    enabled: false  # We'll mount DAGs from ConfigMap for now
  gitSync:
    enabled: false  # TODO: Enable in production with git repo

# Logs persistence
logs:
  persistence:
    enabled: false  # Use emptyDir for local dev

# ServiceAccount
serviceAccount:
  create: true
  name: airflow

# RBAC for KubernetesExecutor
createUserJob:
  useHelmHooks: false

migrateDatabaseJob:
  useHelmHooks: false

# Resource requests/limits for task pods
workers:
  persistence:
    enabled: false

# Airflow variables (accessible in DAGs via Variable.get())
airflowLocalSettings: |
  # Custom Python settings
  import os
  
# Extra volumes and volume mounts
extraVolumes: []
extraVolumeMounts: []

# Network policy
networkPolicies:
  enabled: false
